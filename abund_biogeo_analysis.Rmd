---
title: "Abundance_analysis_abundant_protists"
author: "Zavadska Daryna"
date: "2023-06-21"
output: html_document
---

```{r}
getwd()
knitr::opts_knit$set(root.dir = "/home/dzavadska/Data/biogeo_abund")
```


So, the list of abundant protists and their sequences is here:
https://docs.google.com/spreadsheets/d/1s7_ylFshOKtvWx-1DKZWrgGDcF_HiO0gwMUzNwOlmzw/edit?usp=sharing 

```{r}
abpr <- read.csv("the_most_abundant_Protists.csv")

#str(abpr)
```

We will need columns with BEAP_ID, Species_Assignation, Trimmed_18S_Sequence, Tara_V9_md5, Tara_V9_Match_. , Culture_ID

```{r}
abpr_work <- abpr[c(1:(nrow(abpr)-1)),c("BEAP_ID", "Culture_ID", "Species_Assignation", "Trimmed_18S_Sequence", "Tara_V9_md5", "Tara_V9_Match_.")]
```


TARA Oceans V9 metabarcoding abundance data can be downloaded here https://zenodo.org/record/7236051
We will need https://zenodo.org/record/7236051/files/TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz?download=1 

```{r}
#download.file('https://zenodo.org/record/7236051/files/TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz?download=1', destfile = "TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz", method = "wget")
```

tsv.gz files are easy to read using fread function from data.table package
```{r}
library(data.table)
v9data <- fread("TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz")

#str(v9data)
```

Environmental data from Tara Oceans samples is here: https://doi.pangaea.de/10.1594/PANGAEA.875577 
Exactly here https://doi.pangaea.de/10.1594/PANGAEA.875577?format=textfile 
```{r}
#download.file('https://doi.pangaea.de/10.1594/PANGAEA.875577?format=textfile', destfile = "TARA_CONTEXT_non-trunc.csv", method = "wget")
#Note - for primary editing, it is important to use ONLY TAB as a separator!!
context <- fread("TARA_CONTEXT.csv")
```

```{r}
colnames(context)
head(context)
```

# Estimating Relative abundances of selected barcode (method 1)

Selected barcode
```{r}
md5sum <- abpr_work$Tara_V9_md5[9]
```

Abundances of the given barcode
```{r}
protist_ab <- as.numeric(v9data[which(v9data$amplicon==md5sum),6:ncol(v9data)])
```

Getting relative abundances of barcode of interest by summing up the total read abundance per sample and dividing abundances of the barcode by the total read abundance in each sample.
Here we use function apply; we could have used the "for" loop as well, but this consumes more computational resource, and with our dataset size this is a big problem.
```{r}
v9data <- as.data.frame(v9data)[,c(1,6:ncol(v9data))]

colsums <- apply( v9data[2:ncol(v9data)] , 2, sum)

protist_rel_ab <- protist_ab/as.numeric(colsums)
#fwrite(as.data.frame(protist_ab), "protist_rel_ab.csv")
sum(protist_rel_ab)
```
# Investigating Tara context data

The aforementioned TARA context data has too many variables; in our analysis, we will stick to the parameters described in the analysis here https://www.nature.com/articles/s41396-018-0340-5#Sec2 ;  this data can be uploaded from here
https://figshare.com/articles/dataset/Data_MixoBioGeo_Faure_et_al_2018/6715754?file=12249194 


```{r}
env_subs <- read.csv("Envi_context_Faure_et_al_2018.txt", sep = " ")
```

We could have used env_subs directly, but the metabarcoding data has abundance for "Sample.ID"-this data is present in "extended original" context dataset, but it is absent from the smaller dataset.
Check if sample.mat is unique or duplicated, and do the same with Sample ID

```{r}
length(unique(context$`Sample ID (TARA_barcode#, registered at ...)`))
length(unique(context$`Samp mat (TARA_station#_environmental-f...)`))
```
We just found out there is no actual way to find nonambigous correspondence between Sample.mat and Sample.ID. This is because same sample.mat can occur in >1 Sample.ID. This sucks. People get around either by passing a sacred vocabulary, which is kept by one of Tara bioinformaticians, to one another, the way we will do just now.

Reading the vocabulary
```{r}
vocabulary <- read.csv("TAGs-18S-V4_NAME-PROJ-MAT-READ-CAB_nico.list", sep = "\t", header = FALSE)
```


Luckily, from my past work work I got a vocabulry a piece of code adopted to match sample.mat and sample.id

```{r}
library(stringr)
    tmp <- context
colnames(tmp) <- str_replace(str_squish(str_replace(colnames(context), "[(](.+)[)]", "")), " ", ".")
#str_replace(str_squish(str_replace(colnames(context), "[(](.+)|(.+)[)]", "")), " ", ".")
    tmp <- tmp[,list(Sample.ID,Samp.mat)]
    tmp[,Sample.ID:=sub("TARA_","",Sample.ID)]
    tmp <- merge(vocabulary,tmp,by.x="V5",by.y="Sample.ID",all.x=T)
    tmp <- tmp[,c("V5","Samp.mat")]
    tmp <- unique(tmp)
    tmp <- tmp[!duplicated("V5")]
  head(tmp)
 # unique(tmp$V5)
 # unique(tmp$Samp.mat)
```

Now we can add the sample name with env_subs and continue with a clean, reasonable dataset on the environmental variables that can matter

```{r}
tmp$V5 <- paste0("TARA_", tmp$V5)
trunc_names <- as.data.frame(str_split(tmp$Samp.mat,"_", simplify = TRUE)[,1:3])
tmp$names <- paste0(trunc_names$V1, "_", trunc_names$V2, "_", trunc_names$V3)
env_subs$names <- rownames(env_subs)
mergeed_1 <- merge(tmp, env_subs)
```


Creating a data of protist relative abundance with corresponding station names; 
putting the abundance of our barcode with the encironmental parameters in one table

```{r}
df_rel_ab <-data.frame("abundance" = protist_rel_ab, "V5" = colnames(v9data)[-1])
for_rda <- merge(mergeed_1, df_rel_ab)
```

# Analysis - first glance


https://www.nature.com/articles/s42003-022-03939-z#Sec10 - this article +/- reflects what we want to do.

It says "Pearson’s correlations between the relative abundance of P. calceolata and all environmental parameters were calculated with the cor function the R package FactoMineR version 2.4 and the GGally package version 2.1.0. Principal component analysis (PCA) was performed with 9 parameters presenting significant Pearson’s correlations. We used a Generalized Additive Model (GAM) for its ability to fit non-linear and non-monotonic functions and for its low sensitivity to extreme values to model the relative abundance of P. calceolata as a function of iron concentration, temperature and PAR light87. This function is implemented in the mgcv R package version 1.8–33."

However, many questions arise in terms of exact stats to be applied, and the chapter available here https://drive.google.com/file/d/1gr_Z_SsmHpjjoJwkc3HQF-CEE9pgJoym/view?usp=drive_link nicely resolves the main concerns.

Problem 1 - data of different types (factor, integer, or float reflecting relative values).

```{r}
head(for_rda)

apply(for_rda[,14:ncol(for_rda)], 2, hist)
```
the abovementioned article says:
"In variables presented as proportions or ratios, e.g., humidity, this problem can be overcome with the arc-sin transformation. In those variables stretched over a large scale of values, e.g., illumination and passage cross section, this can be achieved by transformation in the logarithmic scale."

Here ther is also a good description of PCA https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202#d1e242 , and they suggest, instead of doing fancy log-ratio or arcsin transformations, to do it cheap and fast:
"To overcome this undesirable feature, it is common practice to begin by standardizing the variables. Each data value xij is both centred and divided by the standard deviation sj of the n observations of variable j. Thus, the initial data matrix X is replaced with the standardized data matrix Z, whose jth column is vector zj with the n standardized observations of variable j"

```{r}
#log(for_rda[,10:ncol(for_rda)])
#apply(for_rda[,10:ncol(for_rda)], 2, log)
normalize <- function(x){
  return((x - mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE))
}

transf <- apply(for_rda[,14:ncol(for_rda)], 2, normalize)
apply(transf, 2, hist)
```

Alternatively, we can try scale() function

```{r}
#apply(scale(for_rda[,12:ncol(for_rda)]), 2, hist)
#transf <- scale(for_rda[,12:ncol(for_rda)])
```


Now we can try to see if the distribution of out data is anywhere close to normal. We'll use K.-S. test, which is basically shapiro-wilks test but for bigger datasets. The critical value of this test for n>35 is ~1. 

```{r}
kstest <- apply(transf, 2, ks.test, y="pnorm")

kstest[[10]]$p.value

pv <- c()
for (i in 1:ncol(transf)) {
  pv <- c(pv, (kstest[[i]]$p.value))
}
pv

stats <- c()
for (i in 1:ncol(transf)) {
  stats <- c(stats, (kstest[[i]]$statistic))
}
stats
```

To make a "positive control" to see which test results we get from the actual normally distributed dta, we can create an artificial normal distribution and test it.
```{r}
control <- pnorm(seq(-5, 5, by = 0.05) )

ks.test(control, "pnorm")
```


Covariance matrix

```{r}
library(ggcorrplot)
corr_matrix <- cor(na.exclude(for_rda[,12:ncol(for_rda)]), use = "pairwise.complete.obs")
ggcorrplot(corr_matrix)

corr_matrix[,"abundance"][order(corr_matrix[,"abundance"], decreasing = TRUE)]
#library(GGally)
#ggpairs(data = as.data.frame(transf))
```

PCA
```{r}
library(pls)
transf_df <- as.data.frame(transf)
na.exclude(transf_df)
fit1 <- lm(abundance ~ ., data= na.exclude(transf_df))
summary(fit1)
?lm


transf_df_pca <- transf_df[which(for_rda$abundance != 0),]

?pcr
fit <- pcr( abundance ~ ., data = as.data.frame(transf_df_pca), scale = FALSE, validation = "CV" )
fit <- pcr( abundance ~ ., data = as.data.frame(for_rda[,12:ncol(for_rda)]), scale = FALSE, validation = "CV" )
summary(fit)
validationplot(fit, val.type = 'MSEP' )
min.pcr = which.min( MSEP( fit )$val[1,1, ] ) - 1
coef(fit, ncomp = min.pcr)


data.pca <- princomp(corr_matrix)
summary(data.pca)
```

Regression 
```{r}
fit_1 <- lm(abundance ~ ., data = as.data.frame(transf))
summary(fit_1)
```




################################33

```{r}
cor(as.matrix(for_rda[,10:ncol(for_rda)]), as.matrix(for_rda[,ncol(for_rda)]), use = "pairwise.complete.obs")

cor(as.matrix(for_rda[,10]), as.matrix(for_rda[,ncol(for_rda)]), use = "pairwise.complete.obs")
library("corrplot")
corrplot(cor.mat, type="upper", order="hclust", 
         tl.col="black", tl.srt=45)


dim(as.matrix(for_rda[,ncol(for_rda)]))
dim(for_rda[,10:ncol(for_rda)])

apply(as.matrix(for_rda[,10:ncol(for_rda)]), 2, cor(x,  for_rda[,ncol(for_rda)]))

?cor
```























##################################

Getting relative abundances of everything - 0 replacement and log-ratio transformation. This script takes too much memory, it is computationally demanding. So, to perform this transformation, I created a separate script, run it on my local server 
in the lab, and created a new file with log-transformed counts.
```{r}
###iqlr transformation function from propr package
##
ivar2index <- function(counts){
  ct <- counts
  if(any(counts == 0)){
    message("Alert: Replacing 0s with next smallest value.")
    zeros <- counts == 0
    counts[zeros] <- min(counts[!zeros])
  }
  
  counts.clr <- apply(log(counts), 1, function(x){ x - mean(x) })##
  counts.var <- apply(counts.clr, 1, var)##
  quart <- stats::quantile(counts.var) # use features with unextreme variance
  use <- which(counts.var < quart[4] & counts.var > quart[2])
  
  
  #return(use)
  #here the original "ivar2index" function piece from propr package ends - this piece is intended to define a reference used for iqlr transformation (below)
  
  #here iqlr transformation begins
  message("Alert: Saving log-ratio transformed counts to @logratio.")
  logX <- log(counts)
  logSet <- logX[, use, drop = FALSE]
  ref <- rowMeans(logSet)
  lr <- sweep(logX, 1, ref, "-")
  
  return(lr)
}
################################

#abpr_work_iqlr <- ivar2index(v9data[,6:ncol(v9data)])
```

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
```{r}
dcast(small, amplicon~variable, value.var = "value")
```


```{r}

v9data_nochr <-v9data[2:ncol(v9data)]
zeros <- v9data_nochr == 0
v9data_nochr[zeros] <- min(v9data_nochr[!zeros])
  
  v9data.clr <- apply(log(v9data_nochr), 1, function(x){ x - mean(x) })##
  v9data.var <- apply(v9data.clr, 1, var)##
  quart <- stats::quantile(v9data.var) # use features with unextreme variance
  use <- which(v9data.var < quart[4] & v9data.var > quart[2])
  
  logX <- log(v9data_nochr)
  logSet <- logX[use, , drop = FALSE]
  ref <- colMeans(logSet)
  lr <- sweep(logX, 1, ref, "-")
```



########################################


# Analysis - corncob

```{r}
library(phyloseq)
data(soil_phylum_small)
soil <- soil_phylum_small
otu_table(soil)[1:5,]
```


```{r}
otu_mat<- as.data.frame(for_rda[,c(1,ncol(for_rda))])

tax_mat<- read_excel("../data/CARBOM data.xlsx", sheet = "Taxonomy table")
samples_df <- as.matrix(for_rda[,12:(ncol(for_rda)-1)])
  otu_mat <- otu_mat %>% 
    tibble::column_to_rownames("V5")

OTU = otu_table(otu_mat, taxa_are_rows = FALSE)

```



```{r}
library(corncob)

bbdml()
```











