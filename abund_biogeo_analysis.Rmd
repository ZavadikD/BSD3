---
title: "Abundance_analysis_abundant_protists"
author: "Zavadska Daryna"
date: "2023-06-21"
output: html_document
---

```{r}
knitr::opts_knit$set(root.dir = "/home/dzavadska/Data/biogeo_abund")
```


So, the list of abundant protists and their sequences is here:
https://docs.google.com/spreadsheets/d/1s7_ylFshOKtvWx-1DKZWrgGDcF_HiO0gwMUzNwOlmzw/edit?usp=sharing 

```{r}
abpr <- read.csv("the_most_abundant_Protists.csv")

#str(abpr)
```

We will need columns with BEAP_ID, Species_Assignation, Trimmed_18S_Sequence, Tara_V9_md5, Tara_V9_Match_. , Culture_ID

```{r}
abpr_work <- abpr[c(1:(nrow(abpr)-1)),c("BEAP_ID", "Culture_ID", "Species_Assignation", "Trimmed_18S_Sequence", "Tara_V9_md5", "Tara_V9_Match_.")]
```


TARA Oceans V9 metabarcoding abundance data can be downloaded here https://zenodo.org/record/7236051
We will need https://zenodo.org/record/7236051/files/TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz?download=1 

```{r}
#download.file('https://zenodo.org/record/7236051/files/TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz?download=1', destfile = "TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz", method = "wget")
```

tsv.gz files are easy to read using fread function from data.table package
```{r}
library(data.table)
v9data <- fread("TARA-Oceans_18S-V9_Swarm-Mumu_table.tsv.gz")

#str(v9data)
```

Environmental data from Tara Oceans samples is here: https://doi.pangaea.de/10.1594/PANGAEA.875577 
Exactly here https://doi.pangaea.de/10.1594/PANGAEA.875577?format=textfile 
```{r}
#download.file('https://doi.pangaea.de/10.1594/PANGAEA.875577?format=textfile', destfile = "TARA_SAMPLES_CONTEXT_ENV-WATERCOLUMN.tab", method = "wget")

context <- fread("TARA_CONTEXT.csv")
```

```{r}
colnames(context)
head(context)
```

# Estimating Relative abundances of selected barcode (method 1)

Selected barcode
```{r}
md5sum <- abpr_work$Tara_V9_md5[9]
```

Abundances of the given barcode
```{r}
protist_ab <- as.numeric(v9data[which(v9data$amplicon==md5sum),6:ncol(v9data)])
```

Getting relative abundances of barcode of interest by summing up the total read abundance per sample and dividing abundances of the barcode by the total read abundance in each sample.
Here we use function apply; we could have used the "for" loop as well, but this consumes more computational resource, and with our dataset size this is a big problem.
```{r}
v9data <- as.data.frame(v9data)[,c(1,6:ncol(v9data))]

colsums <- apply( v9data[2:ncol(v9data)] , 2, sum)

protist_rel_ab <- protist_ab/as.numeric(colsums)

sum(protist_rel_ab)
```
# Investigating Tara context data

The aforementioned TARA context data has too many variables; in our analysis, we will stick to the parameters described in the analysis here https://www.nature.com/articles/s41396-018-0340-5#Sec2 ;  this data can be uploaded from here
https://figshare.com/articles/dataset/Data_MixoBioGeo_Faure_et_al_2018/6715754?file=12249194 


```{r}
env_subs <- read.csv("Envi_context_Faure_et_al_2018.txt", sep = " ")
```

We could have used env_subs directly, but the metabarcoding data has abundance for "Sample.ID"-this data is present in "extended original" context dataset, but it is absent from the smaller dataset.
Check if sample.mat is unique or duplicated, and do the same with Sample ID

```{r}
length(unique(context$`Sample ID (TARA_barcode#, registered at ...)`))
length(unique(context$`Samp mat (TARA_station#_environmental-f...)`))
```
We just found out there is no actual way to find nonambigous correspondence between Sample.mat and Sample.ID. This is because same sample.mat can occur in >1 Sample.ID. This sucks. People get around either by passing a sacred vocabulary, which is kept by one of Tara bioinformaticians, to one another, the way we will do just now.

Reading the vocabulary
```{r}
vocabulary <- read.csv("TAGs-18S-V4_NAME-PROJ-MAT-READ-CAB_nico.list", sep = "\t", header = FALSE)
```


Luckily, from my past work work I got a vocabulry a piece of code adopted to match sample.mat and sample.id

```{r}
library(stringr)
    tmp <- context
colnames(tmp) <- str_replace(str_squish(str_replace(colnames(context), "[(](.+)[)]", "")), " ", ".")
    tmp <- tmp[,list(Sample.ID,Samp.mat)]
    tmp[,Sample.ID:=sub("TARA_","",Sample.ID)]
    tmp <- merge(vocabulary,tmp,by.x="V5",by.y="Sample.ID",all.x=T)
    tmp <- tmp[,c("V5","Samp.mat")]
    tmp <- unique(tmp)
    tmp <- tmp[!duplicated("V5")]
  head(tmp)
```

Now we can add the sample name with env_subs and continue with a clean, reasonable dataset on the environmental variables that can matter

```{r}
tmp$V5 <- paste0("TARA_", tmp$V5)
mergeed_1 <- merge(tmp, env_subs)
```


Creating a data of protist relative abundance with corresponding station names; 
putting the abundance of our barcode with the encironmental parameters in one table

```{r}
df_rel_ab <-data.frame("abundance" = protist_rel_ab, "V5" = colnames(v9data)[-1])
for_rda <- merge(mergeed_1, df_rel_ab)
```

# Analysis - first glance





# Analysis - PCA 
https://www.nature.com/articles/s42003-022-03939-z#Sec10 - this article +/- reflects what we want to do.

It says "Pearson’s correlations between the relative abundance of P. calceolata and all environmental parameters were calculated with the cor function the R package FactoMineR version 2.4 and the GGally package version 2.1.0. Principal component analysis (PCA) was performed with 9 parameters presenting significant Pearson’s correlations. We used a Generalized Additive Model (GAM) for its ability to fit non-linear and non-monotonic functions and for its low sensitivity to extreme values to model the relative abundance of P. calceolata as a function of iron concentration, temperature and PAR light87. This function is implemented in the mgcv R package version 1.8–33."

```{r}
cor(as.matrix(for_rda[,10:ncol(for_rda)]), as.matrix(for_rda[,ncol(for_rda)]), use = "pairwise.complete.obs")

cor(as.matrix(for_rda[,10]), as.matrix(for_rda[,ncol(for_rda)]), use = "pairwise.complete.obs")
library("corrplot")
corrplot(cor.mat, type="upper", order="hclust", 
         tl.col="black", tl.srt=45)


dim(as.matrix(for_rda[,ncol(for_rda)]))
dim(for_rda[,10:ncol(for_rda)])

apply(as.matrix(for_rda[,10:ncol(for_rda)]), 2, cor(x,  for_rda[,ncol(for_rda)]))

?cor
```























##################################

Getting relative abundances of everything - 0 replacement and log-ratio transformation. This script takes too much memory, it is computationally demanding. So, to perform this transformation, I created a separate script, run it on my local server 
in the lab, and created a new file with log-transformed counts.
```{r}
###iqlr transformation function from propr package
##
ivar2index <- function(counts){
  ct <- counts
  if(any(counts == 0)){
    message("Alert: Replacing 0s with next smallest value.")
    zeros <- counts == 0
    counts[zeros] <- min(counts[!zeros])
  }
  
  counts.clr <- apply(log(counts), 1, function(x){ x - mean(x) })##
  counts.var <- apply(counts.clr, 1, var)##
  quart <- stats::quantile(counts.var) # use features with unextreme variance
  use <- which(counts.var < quart[4] & counts.var > quart[2])
  
  
  #return(use)
  #here the original "ivar2index" function piece from propr package ends - this piece is intended to define a reference used for iqlr transformation (below)
  
  #here iqlr transformation begins
  message("Alert: Saving log-ratio transformed counts to @logratio.")
  logX <- log(counts)
  logSet <- logX[, use, drop = FALSE]
  ref <- rowMeans(logSet)
  lr <- sweep(logX, 1, ref, "-")
  
  return(lr)
}
################################

#abpr_work_iqlr <- ivar2index(v9data[,6:ncol(v9data)])
```



```{r}

v9data_nochr <-v9data[2:ncol(v9data)]
zeros <- v9data_nochr == 0
v9data_nochr[zeros] <- min(v9data_nochr[!zeros])
  
  v9data.clr <- apply(log(v9data_nochr), 1, function(x){ x - mean(x) })##
  v9data.var <- apply(v9data.clr, 1, var)##
  quart <- stats::quantile(v9data.var) # use features with unextreme variance
  use <- which(v9data.var < quart[4] & v9data.var > quart[2])
  
  logX <- log(v9data_nochr)
  logSet <- logX[use, , drop = FALSE]
  ref <- colMeans(logSet)
  lr <- sweep(logX, 1, ref, "-")
```






# Analysis - corncob

```{r}
library(phyloseq)
data(soil_phylum_small)
soil <- soil_phylum_small
otu_table(soil)[1:5,]
```
















